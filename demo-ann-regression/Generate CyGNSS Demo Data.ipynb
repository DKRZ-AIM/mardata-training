{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1619)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = h5py.File('/work/ka1176/shared_data/2020-03/dev_data/dev_data_random_all_map/train_data.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = source_file['ddm_timestamp_unix'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windspeed = source_file['windspeed'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daystamp = ((timestamp - timestamp[0]) / 24 / 3600).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uvals, ucnts = np.unique(daystamp, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(uvals, ucnts)\n",
    "plt.vlines(120, 0, 250000)\n",
    "plt.vlines(200, 0, 250000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target data set sizes:\n",
    "    - 60,000 samples for training\n",
    "    - 10,000 samples for validation\n",
    "    - 10,000 samples for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_train = np.random.choice(np.sum(daystamp<120), size=60000, replace=False)\n",
    "ix_valid = np.random.choice(np.sum((daystamp>=120) & (daystamp<200)), size=10000, replace=False) + np.sum(daystamp<120)\n",
    "ix_test  = np.random.choice(np.sum(daystamp>=200), size=10000, replace=False) + np.sum(daystamp<200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(windspeed[ix_train], label='Train')\n",
    "sns.distplot(windspeed[ix_valid], label='Valid')\n",
    "sns.distplot(windspeed[ix_test], label='Test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(daystamp[ix_train], label='Train')\n",
    "sns.distplot(daystamp[ix_valid], label='Valid')\n",
    "sns.distplot(daystamp[ix_test], label='Test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables in the new data set\n",
    "- brcs (17 x 11 map)\n",
    "- eff_scatter (17 x 11 map)\n",
    "- quality (bool: select)\n",
    "- windspeed (target)\n",
    "- ddm_nbrcs (for the linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load all features and targets\n",
    "train_dataset = dict()\n",
    "valid_dataset = dict()\n",
    "test_dataset = dict()\n",
    "\n",
    "brcs = source_file['brcs'][:]\n",
    "train_dataset['brcs'] = brcs[ix_train]\n",
    "valid_dataset['brcs'] = brcs[ix_valid]\n",
    "test_dataset['brcs'] = brcs[ix_test]\n",
    "del brcs\n",
    "\n",
    "eff_scatter = source_file['eff_scatter'][:]\n",
    "train_dataset['eff_scatter'] = eff_scatter[ix_train]\n",
    "valid_dataset['eff_scatter'] = eff_scatter[ix_valid]\n",
    "test_dataset['eff_scatter'] = eff_scatter[ix_test]\n",
    "del eff_scatter\n",
    "\n",
    "quality = source_file['ddm_nbrcs_exceeds_alpha_0.975'][:].astype(bool) & ~source_file['ddm_nbrcs_exceeds_alpha_0.025'][:].astype(bool)\n",
    "train_dataset['quality'] = quality[ix_train]\n",
    "valid_dataset['quality'] = quality[ix_valid]\n",
    "test_dataset['quality'] = quality[ix_test]\n",
    "\n",
    "ddm_nbrcs = source_file['ddm_nbrcs'][:]\n",
    "train_dataset['ddm_nbrcs'] = ddm_nbrcs[ix_train]\n",
    "valid_dataset['ddm_nbrcs'] = ddm_nbrcs[ix_valid]\n",
    "test_dataset['ddm_nbrcs'] = ddm_nbrcs[ix_test]\n",
    "\n",
    "windspeed = source_file['windspeed'][:]\n",
    "train_dataset['windspeed'] = windspeed[ix_train]\n",
    "valid_dataset['windspeed'] = windspeed[ix_valid]\n",
    "test_dataset['windspeed'] = windspeed[ix_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes=dict(quality='bool', windspeed='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset, h5_file_name):\n",
    "    if os.path.exists(h5_file_name):\n",
    "        os.remove(h5_file_name)\n",
    "        print('cleared', h5_file_name)\n",
    "    h5_file = h5py.File(h5_file_name, 'w')\n",
    "    image_dims = dataset['brcs'][0].shape\n",
    "    n_samples = len(dataset['windspeed'])\n",
    "    if n_samples > 0:\n",
    "        for key in dataset.keys():\n",
    "            chunk_size = 1000 if n_samples >= 1000 else n_samples\n",
    "            if key in ['brcs', 'eff_scatter']:\n",
    "                h5_file.create_dataset(key,\n",
    "                                       shape=(n_samples,) + image_dims,\n",
    "                                       chunks=(chunk_size,) + image_dims,\n",
    "                                       fletcher32=True,\n",
    "                                       dtype='float32')\n",
    "            else:\n",
    "                h5_file.create_dataset(key,\n",
    "                                       shape=(n_samples,),\n",
    "                                       chunks=(chunk_size,),\n",
    "                                       fletcher32=True,\n",
    "                                       dtype=dtypes[key])\n",
    "            h5_file[key][:] = dataset[key]\n",
    "            h5_file.flush()\n",
    "    h5_file.attrs['timestamp'] = str(datetime.datetime.now())\n",
    "    h5_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to three *.h5 files\n",
    "save_dataset(train_dataset, './data_regression_cygnss/train_data.h5')\n",
    "save_dataset(valid_dataset, './data_regression_cygnss/valid_data.h5')\n",
    "save_dataset(test_dataset, './data_regression_cygnss/test_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 bleeding edge (using the module anaconda3/bleeding_edge)",
   "language": "python",
   "name": "anaconda3_bleeding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
