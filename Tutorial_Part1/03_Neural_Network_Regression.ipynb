{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook', font_scale=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1703) # we fix the random number generators here for the purpose of reproducibility\n",
    "np.random.seed(1703)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we want to use supervised learning for establishing a neural network that can predict wind speed from CyGNSS observations (see lecture). Since wind speed is a continuous variable, this is a *regression* problem. We have prepared training, validation, and test data. \n",
    "\n",
    "Your task is to find the best possible neural network for predicting wind speed. We will walk you through various choices you can make to the data and the model algorithm. You will train various models and compare their performance on the validation dataset. Finally, you will choose your best model and make predictions on the test dataset.\n",
    "\n",
    "A sample consists of *input features* and a *target label*. One such sample is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('./demo_sample.png', height=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural network in pytorch requires setting up several components:\n",
    "- data set: makes your raw data set accessible to pytorch\n",
    "- data loader: fetches data in batches\n",
    "- neural network architecture\n",
    "- method for training the network\n",
    "- method for making predictions with the network\n",
    "    \n",
    "For the purpose of this tutorial, it is not necessary to understand all these components in detail, however, they are documented and explained for later reference. You may execute the following cells up to \"Interactive tutorial\" and start with exploring the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pytorch dataset and data loader are discussed in more detail in the workshop on data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyGNSSDataset(Dataset):\n",
    "    def __init__(self, flag, input_v_map=['brcs'], normalization_values=None, filter_quality=False):\n",
    "        '''\n",
    "        Load data and apply transforms during setup\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        flag : string\n",
    "            Any of train / valid / test. Defines dataset.\n",
    "        input_v_map : list\n",
    "            Input maps, choice of ['brcs', 'eff_scatter']\n",
    "        normalization_values : dict\n",
    "            Mean and standard deviation, needed for scaling the input variables\n",
    "        filter_quality : bool\n",
    "            Filter samples that are flagged as bad quality (default: False)\n",
    "        -----------\n",
    "        Returns: dataset\n",
    "        '''\n",
    "        self.h5_file = h5py.File(os.path.join('/work/ka1176/shared_data/CyGNSS/', flag + '_data.h5'), 'r', rdcc_nbytes=0)  # disable cache\n",
    "        # load everything into memory\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # load labels\n",
    "        self.y = self.h5_file['windspeed'][:].astype(np.float32)\n",
    "\n",
    "        # normalize main input data\n",
    "        # Save normalization values together with the trained model\n",
    "        # For inference load the normalization values\n",
    "\n",
    "        if flag=='train': # determine normalization values\n",
    "            self.normalization_values = dict()\n",
    "        else:\n",
    "            self.normalization_values = normalization_values\n",
    "        \n",
    "        # stack map vars (2D vars)\n",
    "        self.X = []\n",
    "        for v_map in input_v_map:\n",
    "            X_v_map = self.h5_file[v_map][:].astype(np.float32)\n",
    "            \n",
    "            if flag=='train':\n",
    "                norm_vals = dict()\n",
    "                X_v_map_scaled, X_mean, X_std = self._standard_scale(X_v_map)\n",
    "                self.normalization_values[f'{v_map}_mean'] = X_mean\n",
    "                self.normalization_values[f'{v_map}_std']  = X_std\n",
    "            else:\n",
    "                X_mean = self.normalization_values[f'{v_map}_mean']\n",
    "                X_std = self.normalization_values[f'{v_map}_std']\n",
    "                X_v_map_scaled = self._standard_scale_given(X_v_map, X_mean, X_std)\n",
    "                \n",
    "            self.X.append(X_v_map_scaled) # append scaled 2D map\n",
    "            #self.X.append(X_v_map) # append unscaled 2D map (test)\n",
    "        self.X = np.stack(self.X, axis=1)\n",
    "        \n",
    "        if filter_quality:\n",
    "            n_before = len(self.y)\n",
    "            mask = self.h5_file['quality'][:]\n",
    "            self.X, self.y = self.X[mask], self.y[mask]\n",
    "            print(f'After filter_quality, {len(self.y)} samples remain ({len(self.y)/n_before*100:.1f}%)')\n",
    "\n",
    "        print(f'load and transform {flag} input data: {self.X.shape} ({self.X.nbytes // 1e6}MB)')\n",
    "        print(f'load and transform {flag} labels: {self.y.shape} ({self.y.nbytes // 1e6}MB)')\n",
    "        \n",
    "    def _standard_scale(self, v):\n",
    "        '''apply standard scale and return mean / std'''\n",
    "        mean = np.mean(v)\n",
    "        sigma = np.std(v)\n",
    "        v_tilde = (v - mean) / sigma\n",
    "        return v_tilde, mean, sigma\n",
    "    \n",
    "    def _standard_scale_given(self, v, mean, sigma):\n",
    "        '''apply standard scale with pre-determined mean / std'''\n",
    "        v_tilde = (v - mean) / sigma\n",
    "        return v_tilde\n",
    "\n",
    "    def _filter_all_data_by_mask(self, mask, flag, name=''): \n",
    "        '''filter the input data by the provided mask'''\n",
    "        self.X, self.y = self.X[mask], self.y[mask]\n",
    "        print(f'{flag} input data after {name} downsampling: {self.X.shape} ({self.X.nbytes // 1e6}MB)')\n",
    "\n",
    "    def __len__(self):\n",
    "        '''required function for the pytorch dataloader'''\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''required function for the pytorch dataloader'''\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return (X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataloaders(filter_quality=False, input_v_map=['brcs']):\n",
    "    '''Load the datasets and create PyTorch dataloaders\n",
    "    \n",
    "    Input parameters:\n",
    "    -------------------------\n",
    "    filter_quality : apply a filter for sample quality (default: False)\n",
    "    input_v_map    : list of input features (default: ['brcs'])\n",
    "    -------------------------\n",
    "    \n",
    "    Returns:\n",
    "    -------------------------\n",
    "    pytorch DataLoader instances for train / validation / test set\n",
    "    '''\n",
    "    \n",
    "    train_dataset = CyGNSSDataset('train', filter_quality=filter_quality, input_v_map=input_v_map)\n",
    "    valid_dataset = CyGNSSDataset('valid', filter_quality=filter_quality, input_v_map=input_v_map, normalization_values=train_dataset.normalization_values)\n",
    "    test_dataset = CyGNSSDataset('test', filter_quality=filter_quality, input_v_map=input_v_map, normalization_values=train_dataset.normalization_values)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    test_dataloader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pre-defined two network architectures for this tutorial:\n",
    "- Fully connected network\n",
    "- Convolutional neural network\n",
    "\n",
    "At initialization, the individual network components are defined (layers like nn.Linear, nn.Dropout, nn.Conv2d). In the forward function, the forward pass through the network is defined, i.e., the order in which the layers are applied to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNetwork(nn.Module):\n",
    "    '''A fully connected neural network\n",
    "    \n",
    "    Input parameters:\n",
    "    ----------------------\n",
    "    units_dense1 : first layer dimension\n",
    "    units_dense2 : second layer dimension\n",
    "    dropout      : dropout value following the second layer\n",
    "    '''\n",
    "    def __init__(self, units_dense1=64, units_dense2=32, dropout=0, input_shape=(1, 17, 11)):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        # first dense layer\n",
    "        self.fc1 = nn.Linear(np.product(input_shape), units_dense1)\n",
    "        # second dense layer\n",
    "        self.fc2 = nn.Linear(units_dense1, units_dense2)\n",
    "        # dropout layer for regularization\n",
    "        self.dr_fc2 = nn.Dropout(dropout)\n",
    "        # final layer producing the output\n",
    "        self.fc_final = nn.Linear(units_dense2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the input images to 1D arrays\n",
    "        x = torch.flatten(x, 1)\n",
    "        # first layer\n",
    "        x = self.fc1(x)\n",
    "        # activation function\n",
    "        x = F.relu(x)\n",
    "        # second layer\n",
    "        x = self.fc2(x)\n",
    "        # activation function\n",
    "        x = F.relu(x)\n",
    "        # dropout layer\n",
    "        x = self.dr_fc2(x)\n",
    "        # output layer\n",
    "        x = self.fc_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    '''A convolutional neural network with 2 convolutional layers, followed by two dense layers'''\n",
    "    def __init__(self, filters=16, units_dense1=32, units_dense2=16, dropout=0, input_shape=(1, 17, 11)):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.cv1 = nn.Conv2d(input_shape[0], filters, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(filters)\n",
    "        self.pl1 = nn.MaxPool2d(2)\n",
    "        self.cv2 = nn.Conv2d(filters, 2*filters, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(2*filters)\n",
    "        \n",
    "        S = int(filters * 2 * 5 * 8)\n",
    "        \n",
    "        self.fc1 = nn.Linear(S, units_dense1)\n",
    "        self.fc2 = nn.Linear(units_dense1, units_dense2)\n",
    "        self.dr_fc2 = nn.Dropout(dropout)\n",
    "        self.fc_final = nn.Linear(units_dense2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.cv1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.pl1(x)\n",
    "        x = F.relu(self.cv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dr_fc2(F.relu(self.fc2(x)))\n",
    "        x = self.fc_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, valid_dataloader, max_epochs=100, early_stopping=False, patience=3, learning_rate=1e-3, verbose=True):\n",
    "    '''\n",
    "    Train a model.\n",
    "    \n",
    "    model : the model to train\n",
    "    train_dataloader : the dataloader for the train dataset\n",
    "    valid_dataloader : the dataloader for the validation dataset\n",
    "    max_epochs : maximum number of epochs to train (default: 100)\n",
    "    early_stopping : apply the early stopping condition (default: False)\n",
    "    patience : how many epochs to wait for early stopping (default: 3)\n",
    "    learning_rate : optimizer learning rate (default: 0.001)\n",
    "    verbose : print losses after each epoch (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    model : the trained model\n",
    "    train_losses : history of train loss per epoch \n",
    "    valid_losses : history of validation loss per epoch\n",
    "    '''\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    if early_stopping:\n",
    "        best_valid_loss = np.inf\n",
    "        patience_counter = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        epoch_train_loss = []\n",
    "        for batch_idx, (features, target) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features)\n",
    "            output = torch.squeeze(output, dim=1)\n",
    "            loss = F.mse_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss.append(loss.item())\n",
    "        \n",
    "        tl = np.mean(epoch_train_loss)\n",
    "        \n",
    "        # validate\n",
    "        model.eval()\n",
    "        epoch_valid_loss = []\n",
    "        for batch_idx, (features, target) in enumerate(valid_dataloader):\n",
    "            output = torch.squeeze(model(features), dim=1)\n",
    "            loss = F.mse_loss(output, target)\n",
    "            epoch_valid_loss.append(loss.item())\n",
    "        vl = np.mean(epoch_valid_loss)\n",
    "        \n",
    "        # save losses for training history\n",
    "        train_losses.append(tl)\n",
    "        valid_losses.append(vl)\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch}: train loss = {tl:.4f}, valid loss = {vl:.4f}')\n",
    "        \n",
    "        if early_stopping:\n",
    "            if vl < best_valid_loss:\n",
    "                best_valid_loss = vl\n",
    "                best_model = copy.deepcopy(model)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Applying early stopping condition after validation loss did not improve for {patience} epochs.')\n",
    "                model = copy.deepcopy(best_model) # re-instate the best model\n",
    "                break\n",
    "\n",
    "    print(f'Finished training in {epoch+1} epochs and {time.time()-start_time:.1f} seconds.')\n",
    "        \n",
    "    return model, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, include_plot=False):\n",
    "    '''Make and return predictions with a trained model\n",
    "    \n",
    "    Input parameters:\n",
    "    ------------------------\n",
    "    model : trained model\n",
    "    dataloader : dataloader for the dataset that we want to predict on (typically: validation during evaluation / test for the final predictions)\n",
    "    include_plot : generate a 2D histogram plot (default: False)\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    ------------------------\n",
    "    Predictions as a 1D numpy array\n",
    "    '''\n",
    "    model.eval()\n",
    "    predict_loss = []\n",
    "    outputs = []\n",
    "    weights = []\n",
    "    for batch_idx, (features, target) in enumerate(dataloader):\n",
    "            output = torch.squeeze(model(features), dim=1)\n",
    "            loss = F.mse_loss(output, target)\n",
    "            predict_loss.append(loss.item())\n",
    "            outputs.append(output.detach().numpy())\n",
    "            weights.append(len(target) / batch_size)\n",
    "    mse = np.average(predict_loss, weights=weights)\n",
    "    print(f'Prediction: Loss = {mse:.4f}')\n",
    "    print(f'--> RMSE = {np.sqrt(mse):.2f} m/s')\n",
    "    \n",
    "    y_pred = np.concatenate(outputs)\n",
    "    \n",
    "    if include_plot:\n",
    "        plt.hexbin(dataloader.dataset.y[:len(y_pred)], y_pred, mincnt=1, cmap='viridis')\n",
    "        plt.colorbar(label='Sample count')\n",
    "        ax=plt.gca()\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlabel('True wind speed (m/s)')\n",
    "        ax.set_ylabel('Predicted wind speed (m/s)')\n",
    "        ax.plot(range(0, 20), range(0, 20), 'r--')\n",
    "        plt.show()\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network and training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network and training parameters need to be determined individually for each machine learning problem. For this tutorial, we selected reasonable values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128      # number of samples in one mini-batch of training data\n",
    "learning_rate = 1e-3  # ADAM optimizer learning rate\n",
    "max_epochs = 75       # maximum number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = setup_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the first model. This may take few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConnectedNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, train_losses, valid_losses = train_model(model, train_dataloader, valid_dataloader, max_epochs=max_epochs, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we recorded the loss each epoch on the training and the validation set. \n",
    "\n",
    "By executing the following cell, plot the losses as a function of epoch. We see as the training progresses, the loss on the training set decreases (bias is reduced). However, the performance on the validation set does not improve further at some point. Even worse, the validation loss may start to grow again.\n",
    "\n",
    "TASK: Try this out by changing the parameter may_epochs to a larger value, e.g., 150. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, 'o:', label='Train')\n",
    "plt.plot(valid_losses, 'o:', label='Valid')\n",
    "plt.ylim(2, 5.5)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE (Loss)')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strategy to combat overfitting is to employ an Early Stopping condition. We monitor the loss on the validation set, and once it ceases to improve, we interrupt the training process.\n",
    "\n",
    "TASK: With the following cells, train the model again, this time employing the Early Stopping condition. For how many epochs did the network train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FullyConnectedNetwork()\n",
    "model, train_losses, valid_losses = train_model(model, train_dataloader, valid_dataloader, max_epochs=max_epochs, learning_rate=learning_rate, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, 'o:', label='Train')\n",
    "plt.plot(valid_losses, 'o:', label='Valid')\n",
    "plt.ylim(2, 5.5)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE (Loss)')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the predictions and the true values. Ideally, these would lie on the red diagonal line. The root mean squared error (RMSE) is the quantity we use for evaluation. We train different models and compare the RMSE on the validation set to find the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_valid = predict(model, valid_dataloader, include_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the predictions by improving the model and/or improving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the model capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: Experiment with the dimension of the hidden layer by adjusting the values of units_dense1, units_dense2 in the following cell. The default values are units_dense1=64, units_dense2=32 (you may check the definition of the FullyConnectedNetwork above to verify that). \n",
    "\n",
    "As a starting point, try these two combinations:\n",
    "- units_dense1 = 4, units_dense2 = 2\n",
    "- untis_dense1 = 256, units_dense2 = 128\n",
    "\n",
    "How does the dimension of the hidden layer affect the validation loss? Can you find other promising combinations of units_dense1, units_dense2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_dense1 = 64 # change this parameter\n",
    "units_dense2 = 32 # change this parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FullyConnectedNetwork(units_dense1=units_dense1, units_dense2=units_dense2)\n",
    "model, train_losses, valid_losses = train_model(model, train_dataloader, valid_dataloader, max_epochs, early_stopping=True, learning_rate=learning_rate, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_valid = predict(model, valid_dataloader, include_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the quality filter to the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some samples in the data set that ended up having the wrong label. This can happen quite often, and it is one of the main challenges of data processing to improve the dataset quality. In our dataset, there are some samples that have been labeled with the wrong wind speeds. We identified these samples and introduced a mask that we can now use for filtering the samples. More details will be given in Part 2 of our ML workshop.\n",
    "\n",
    "In the next cell, the dataset is reloaded, this time using the flag \"filter_quality=True\". \n",
    "\n",
    "TASK: Reload the dataset and train the model again. Display the network predictions and compare them to the predictions from before. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = setup_dataloaders(filter_quality=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConnectedNetwork(units_dense1=64, units_dense2=32) # you may use better performing parameters from the last exercise\n",
    "model, train_losses, valid_losses = train_model(model, train_dataloader, valid_dataloader, max_epochs, early_stopping=True, learning_rate=learning_rate, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_valid = predict(model, valid_dataloader, include_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add another input feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CyGNSS dataset offers several input features. So far, we used only the feature 'brcs' (the bi-static radar cross section). Now, we add a second input feature 'eff_scatter' (the effective scatter area map), based on the suggestion of a domain scientist. \n",
    "\n",
    "In the data set, we have one more type of input data available. So far, we restricted ourselves to the feature \"brcs\", but now we add the second feature \"eff_scatter\". We are still working with a fully connected network, all input data is flattened and concatenated:\n",
    "\n",
    "Input shape (batch_size, 2, 17, 11) --> (batch_size, 1, 2 * 17 * 11) = (batch_size, 1, 374)\n",
    "\n",
    "TASK: Add the second input feature below. Execute the cells to repeat data set creation and model training. What is the effect on the validation set performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_v_map = ['brcs'] # change this to include eff_scatter as well\n",
    "\n",
    "train_dataloader, valid_dataloader, test_dataloader = setup_dataloaders(filter_quality=True, input_v_map=input_v_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConnectedNetwork(units_dense1=units_dense1, units_dense2=units_dense2, input_shape=(2,17,11))\n",
    "model, train_losses, valid_losses = train_model(model, train_dataloader, valid_dataloader, max_epochs, early_stopping=True, learning_rate=learning_rate, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_valid = predict(model, valid_dataloader, include_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CNN instead of fully connected network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with 2-dimensional input features that look a lot like images, we would like to use a convolutional neural network (CNN). These networks can exploit the relationships between neighboring pixels and can identify and extract relevant features. The number of filters in the convolutional neural network we defined for you can be changed with the parameter filters.\n",
    "\n",
    "TASK: Use the following cells to train a convolutional neural network. Experiment with the number of filters. \n",
    "\n",
    "How does the convolutional neural network perform compared to the fully connected neural network from before? \n",
    "\n",
    "How does the number of filters affect the validation set performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = 32 # change this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ConvolutionalNeuralNetwork(input_shape=(2,17,11), filters=filters)\n",
    "model, train_losses, valid_losses = train_model(model, train_dataloader, valid_dataloader, max_epochs, early_stopping=True, learning_rate=learning_rate, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_valid = predict(model, valid_dataloader, include_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: Finally, choose the model and training parameters that produced the best results on the validation set. Train the model, and use it to make predictions on the test set. Is the loss on the test set comparable to the loss on the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ... # add your model definition here\n",
    "model, train_losses, valid_losses = train_model(model, train_dataloader, valid_dataloader, max_epochs, early_stopping=True, learning_rate=learning_rate, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(model, test_dataloader, include_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Running the notebook took {(time.time()-notebook_start_time)/60} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-2020-03-b",
   "language": "python",
   "name": "my_2020-03"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
