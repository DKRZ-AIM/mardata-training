{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing CyGNSS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO: task with xarray.open_dataset\n",
    "- TODO: task with xarray.open_mfdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/work/ka1176/shared_data/training/CyGNSS-2/train/'\n",
    "all_train_files = [os.path.join(data_dir, ff) for ff in os.listdir(data_dir)]\n",
    "\n",
    "print(len(all_train_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xarray.open_dataset(all_train_files[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds # look at the dataset, jupyter notebook offers an explorer --> click ont he description etxc\n",
    "# Questions: How many samples are in the dataset?\n",
    "# Can you see which type of data is in the datset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Extracting values from a dataset\n",
    "ds.brcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Task: actually loading the dataset\n",
    "x = ds.brcs.values\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task open several files and combine them in one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xarray.open_mfdataset(all_train_files[:10], combine='nested', concat_dim='sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds # how many samples are there now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: demonstrate a dataloader here (one epoch - loop all training samples)\n",
    "# Demonstrate that the order actually matters with lazy loading :-)\n",
    "# Introduce the %%time magic function\n",
    "# Introduce the alternative: start_time = time.time() --> time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = len(ds.sample)\n",
    "batch_size = 128\n",
    "n_batches = N_samples // batch_size # integer division\n",
    "print(n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate one batch\n",
    "start_time = time.time()\n",
    "X = ds.brcs[:batch_size].values\n",
    "y = ds.windspeed[:batch_size].values\n",
    "end_time = time.time()\n",
    "print(f'Execution took {end_time - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Demonstrate one batch using the cell magic function\n",
    "X = ds.brcs.values[:batch_size]\n",
    "y = ds.windspeed.values[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: which method is faster??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(n_batches):\n",
    "    X = ds.brcs[i*batch_size:(i+1)*batch_size].values\n",
    "    y = ds.windspeed[i*batch_size:(i+1)*batch_size].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Get an estimate how long one epoch would take (--> inefficient to read from netcdf files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save as hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: adapt the function xarray-->hdf5 from cygnss git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "    - save the datasets as {train,valid,test}_data.h5 by executing the provided function\n",
    "    - Again try to form a minibatch by randomly reading from these files. Time the execution. Is there a speedup? (--> it is good practice to save the data in a format that is useful for your later application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyGNSSDataset(Dataset):\n",
    "    def __init__(self, flag, input_v_map=['brcs'], normalization_values=None, filter_quality=False):\n",
    "        '''\n",
    "        Load data and apply transforms during setup\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        flag : string\n",
    "            Any of train / valid / test. Defines dataset.\n",
    "        input_v_map : list\n",
    "            Input maps, choice of ['brcs', 'eff_scatter']\n",
    "        normalization_values : dict\n",
    "            Mean and standard deviation, needed for scaling the input variables\n",
    "        filter_quality : bool\n",
    "            Filter samples that are flagged as bad quality (default: False)\n",
    "        -----------\n",
    "        Returns: dataset\n",
    "        '''\n",
    "        self.h5_file = h5py.File(os.path.join('/work/ka1176/shared_data/CyGNSS/', flag + '_data.h5'), 'r', rdcc_nbytes=0)  # disable cache\n",
    "        # load everything into memory\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # load labels\n",
    "        self.y = self.h5_file['windspeed'][:].astype(np.float32)\n",
    "\n",
    "        # normalize main input data\n",
    "        # Save normalization values together with the trained model\n",
    "        # For inference load the normalization values\n",
    "\n",
    "        if flag=='train': # determine normalization values\n",
    "            self.normalization_values = dict()\n",
    "        else:\n",
    "            self.normalization_values = normalization_values\n",
    "        \n",
    "        # stack map vars (2D vars)\n",
    "        self.X = []\n",
    "        for v_map in input_v_map:\n",
    "            X_v_map = self.h5_file[v_map][:].astype(np.float32)\n",
    "            \n",
    "            if flag=='train':\n",
    "                norm_vals = dict()\n",
    "                X_v_map_scaled, X_mean, X_std = self._standard_scale(X_v_map)\n",
    "                self.normalization_values[f'{v_map}_mean'] = X_mean\n",
    "                self.normalization_values[f'{v_map}_std']  = X_std\n",
    "            else:\n",
    "                X_mean = self.normalization_values[f'{v_map}_mean']\n",
    "                X_std = self.normalization_values[f'{v_map}_std']\n",
    "                X_v_map_scaled = self._standard_scale_given(X_v_map, X_mean, X_std)\n",
    "                \n",
    "            self.X.append(X_v_map_scaled) # append scaled 2D map\n",
    "        self.X = np.stack(self.X, axis=1)\n",
    "        \n",
    "        if filter_quality:\n",
    "            n_before = len(self.y)\n",
    "            mask = self.h5_file['quality'][:]\n",
    "            self.X, self.y = self.X[mask], self.y[mask]\n",
    "            print(f'After filter_quality, {len(self.y)} samples remain ({len(self.y)/n_before*100:.1f}%)')\n",
    "\n",
    "        print(f'load and transform {flag} input data: {self.X.shape} ({self.X.nbytes // 1e6}MB)')\n",
    "        print(f'load and transform {flag} labels: {self.y.shape} ({self.y.nbytes // 1e6}MB)')\n",
    "        \n",
    "    def _standard_scale(self, v):\n",
    "        '''apply standard scale and return mean / std'''\n",
    "        mean = np.mean(v)\n",
    "        sigma = np.std(v)\n",
    "        v_tilde = (v - mean) / sigma\n",
    "        return v_tilde, mean, sigma\n",
    "    \n",
    "    def _standard_scale_given(self, v, mean, sigma):\n",
    "        '''apply standard scale with pre-determined mean / std'''\n",
    "        v_tilde = (v - mean) / sigma\n",
    "        return v_tilde\n",
    "\n",
    "    def _filter_all_data_by_mask(self, mask, flag, name=''): \n",
    "        '''filter the input data by the provided mask'''\n",
    "        self.X, self.y = self.X[mask], self.y[mask]\n",
    "        print(f'{flag} input data after {name} downsampling: {self.X.shape} ({self.X.nbytes // 1e6}MB)')\n",
    "\n",
    "    def __len__(self):\n",
    "        '''required function for the pytorch dataloader'''\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''required function for the pytorch dataloader'''\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataloaders(filter_quality=False, input_v_map=['brcs']):\n",
    "    '''Load the datasets and create PyTorch dataloaders\n",
    "    \n",
    "    Input parameters:\n",
    "    -------------------------\n",
    "    filter_quality : apply a filter for sample quality (default: False)\n",
    "    input_v_map    : list of input features (default: ['brcs'])\n",
    "    -------------------------\n",
    "    \n",
    "    Returns:\n",
    "    -------------------------\n",
    "    pytorch DataLoader instances for train / validation / test set\n",
    "    '''\n",
    "    \n",
    "    train_dataset = CyGNSSDataset('train', filter_quality=filter_quality, input_v_map=input_v_map)\n",
    "    valid_dataset = CyGNSSDataset('valid', filter_quality=filter_quality, input_v_map=input_v_map, normalization_values=train_dataset.normalization_values)\n",
    "    test_dataset = CyGNSSDataset('test', filter_quality=filter_quality, input_v_map=input_v_map, normalization_values=train_dataset.normalization_values)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    test_dataloader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "    - iterate through samples by iterating a dataloader (python concept \"yield\")\n",
    "    - pass an argument to the dataset to add another variable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new kernel",
   "language": "python",
   "name": "new-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
