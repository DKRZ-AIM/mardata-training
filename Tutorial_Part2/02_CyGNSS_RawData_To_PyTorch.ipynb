{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing CyGNSS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://xarray.pydata.org/en/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have raw data from the CyGNSS satellite mission that we want to use for a machine learning algorithm. In advance, we decided to use 100 days for training, and 20 days each for validation and test, and sorted the raw data NetCDF files accordingly. Below, we set up the paths to the raw data files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Open a terminal connection to Mistral within Jupyterhub. Change directory to `/work/ka1176/shared_data/training/CyGNSS-2/` and look at the contents of this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '/work/ka1176/shared_data/training/CyGNSS-2/train/'\n",
    "all_train_files = [os.path.join(train_data_dir, ff) for ff in sorted(os.listdir(train_data_dir))]\n",
    "\n",
    "valid_data_dir = '/work/ka1176/shared_data/training/CyGNSS-2/valid/'\n",
    "all_valid_files = [os.path.join(valid_data_dir, ff) for ff in sorted(os.listdir(valid_data_dir))]\n",
    "\n",
    "test_data_dir = '/work/ka1176/shared_data/training/CyGNSS-2/test/'\n",
    "all_test_files = [os.path.join(test_data_dir, ff) for ff in sorted(os.listdir(test_data_dir))]\n",
    "\n",
    "print(f'Number of files for train dataset: {len(all_train_files):3d}')\n",
    "print(f'Number of files for valid dataset: {len(all_valid_files):3d}')\n",
    "print(f'Number of files for test dataset:  {len(all_test_files):3d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open NetCDF files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by opening one of the raw data files and investigate what is there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Open a few different raw data files by changing `day_ix`. What kind of variables are in the NetCDF files? How many samples are in the NetCDF files? Use the interactive explorer in Jupyter notebook to answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_ix = 3\n",
    "ds = xarray.open_dataset(all_train_files[day_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds # Opens an interactive explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from NetCDF files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xarray always uses *lazy loading* for NetCDF files. That means that data is not actually loaded into memory until we explicitly say so. We can use array operations like arithmetic operations, slicing, and subsetting, without loading the data. Only at computation time, the data has to be loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Execute the following two cells. What is the output of each command? \n",
    "\n",
    "HINT: If the cell output is too long, enable scrolling in the cell context menu (right-click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ds.brcs\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = ds.brcs.values\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: The variable `ddm_timestamp` contains a sample timestamp, measured in seconds (verify this by checking the *Attribute* section). Add a new attribute `ddm_day`, which stores the *day*  the sample was recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ddm_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['ddm_day'] = ... # calculate the day here (SOLUTION: (ds.ddm_timestamp / 24 / 3600).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ddm_day.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine several NetCDF files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Use `xarray.open_mfdataset` to open all files at once and form the train / valid / test dataset. How many samples are available in each dataset? Note that `xarray` now loads the input data in *chunks*, instead of loading all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#del ds_train\n",
    "ds_train = xarray.open_mfdataset(all_train_files, combine='nested', concat_dim='sample')\n",
    "ds_valid = ...\n",
    "ds_test  = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data needs to be preprocessed before it can be used in a machine learning algorithm. Below, we demonstrate some typical data cleaning tasks. `xarray` implements many `numpy` functions for its `Dataset` and `DataArray`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK** Remove the missing values from the dataset by executing the following cells. First, we create a `mask`, which is a boolean array. Then, we select only samples that meet the condition of the boolean array. How many samples have been removed? Can you estimate the fraction of None values?\n",
    "\n",
    "Check the documentation of `xarray.ufuncs` and `xarray.Dataset.sel` for more details on these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = xarray.ufuncs.isnan(ds_train.windspeed)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.sel(sample=~mask, drop=True) # note the condition is ~mask (NOT mask): we want to keep NOT none values in the dataset\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for the `brcs` variable. Note this variable has additional dimensions `delay` and `doppler`. We compute the maximum across these dimensions: if any pixel is None, the sample is discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = xarray.ufuncs.isnan(ds_train.brcs)\n",
    "mask = mask.max(dim=['delay', 'doppler']) \n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.sel(sample=~mask, drop=True)\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for fill values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes variables contain fill values, indicating missing raw data that is filled not with `None`, but a finite numeric value. In the CyGNSS dataset, the fill value for the `windspeed` variable is `-1` (verify this by looking at the variable attributes).\n",
    "\n",
    "**TASK**: Remove the samples that have a fill value for `windspeed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ds_train.windspeed==-1\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.sel(sample=~mask, drop=True)\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input feature normalization can be applied during preprocessing or at a later stage (directly before the samples enter the neural network). For demonstration purposes, we apply the normalization right here. We chose min / max normalization, you could apply another normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brcs_max = ds_train.brcs.max().values\n",
    "brcs_min = ds_train.brcs.min().values\n",
    "\n",
    "ds_train['brcs'] -= brcs_min\n",
    "ds_train['brcs'] /= (brcs_max - brcs_min)\n",
    "\n",
    "print(f'Before normalization: max = {brcs_max:.2e}, min = {brcs_min:.2e}')\n",
    "print(f'After normalization:  max = {ds_train.brcs.max().values:.1f}, min = {ds_train.brcs.min().values:.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK** Apply the input feature normalization for the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_valid['brcs'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \"reset\" the ds_train if it is not already done before\n",
    "# ds_train = xarray.open_mfdataset(all_train_files, combine='nested', concat_dim='sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network processes the training data in *minibatches* (see Tutorial, Part 1). Feeding data to the neural network can be a bottleneck for training. In this section you will learn:\n",
    "- How to measure the execution time of code and identify bottlenecks\n",
    "- How to use efficient file formats for machine learning\n",
    "\n",
    "We will use the `brcs` variable as the input feature and the `windspeed` variable as the target variable. A sample is a tuple `(X, y) = (ds['brcs'][i], ds['windspeed'][i])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One epoch of training: cycle all minibatches\n",
    "- One minibatch: collect `batch_size` samples *randomly* from the dataset\n",
    "- Little sidenote: the Python iterator (`yield`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = len(ds_train.sample) # total number of samples\n",
    "batch_size = 128 # typical value for the minibatch size\n",
    "n_batches = N_samples // batch_size # integer division\n",
    "\n",
    "print(f'Train dataset contains {N_samples:.1e} samples')\n",
    "print(f'Batch size {batch_size} ==> {n_batches} minibatches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Execute the cell below to measure the execution time of loading one minibatch. Note this minibatch is not shuffled, we just load the first `batch_size` samples. How long would it take to load the data for a full epoch? How fast is random access compared to accessing the first `batch_size` samples in the previous **TASK**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate one batch\n",
    "start_time = time.time()\n",
    "X = ds_train.brcs[:batch_size].values\n",
    "y = ds_train.windspeed[:batch_size].values\n",
    "end_time = time.time()\n",
    "print(f'Execution took {end_time - start_time:.2e} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_epoch = ... # solution: n_batches * 1.22e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Now we would like to load a shuffled minibatch. How long would it take to load the data for a full epoch this way? \n",
    "\n",
    "Note: We replaced the cumbersome calculation of the execution time by a *cell magic* function `%%time`, that is a nice feature of jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create random indices\n",
    "random_ix = np.random.choice(len(ds_train.sample), size=batch_size, replace=True)\n",
    "X = ds_train.brcs.values[random_ix]\n",
    "y = ds_train.windspeed.values[random_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_epoch = ... # solution: n_batches * 2.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CyGNSS dataset is large, but it still fits comfortably in memory. Therefore, we could also load the full dataset in advance, instead of loading only the samples for one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.nbytes / 1e9 # ds_train size in GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Execute the following cell to load the full dataset in memory. Again, retrieve the time it takes to form one minibatch. How does this time compare to the previous measurements? Note: we are using the magic cell function `%%timeit`, which executes the same code several times and reports mean / std dev of execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_train.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# create random indices\n",
    "random_ix = np.random.choice(len(ds_train.sample), size=batch_size, replace=True)\n",
    "X = ds_train.brcs.values[random_ix]\n",
    "y = ds_train.windspeed.values[random_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_epoch = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataset in hdf5 format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now done with preprocessing the CyGNSS data. It is good practice to separate these steps from your machine learning algorithm. This way, we avoid repeating the preprocessing every time we load training data, and the code is modularized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Use the function `save_ds_hdf5` to save train, valid, and test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ds_to_hdf5(ds, h5_file_name, overwrite=True):\n",
    "    '''\n",
    "    Save a dataset as hdf5.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ds : xarray.Dataset\n",
    "    h5_file_name : target filename\n",
    "    overwrite : if True, overwrite existing files\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    if os.path.exists(h5_file_name):\n",
    "        if overwrite:\n",
    "            print(f'Overwrite {h5_file_name}')\n",
    "            os.remove(h5_file_name)\n",
    "        else:\n",
    "            print(f'Cannot overwrite {h5_file_name}')\n",
    "            return\n",
    "        \n",
    "\n",
    "    h5_file = h5py.File(h5_file_name, 'w')\n",
    "\n",
    "    \n",
    "    n_samples = len(ds.sample)\n",
    "    \n",
    "    h5_file.create_dataset('brcs', \n",
    "                           shape=(n_samples,) + (17,11,),\n",
    "                           chunks=(1000,) + (17,11,),\n",
    "                           fletcher32=True, \n",
    "                           dtype='float32')\n",
    "    \n",
    "    h5_file.create_dataset('windspeed', \n",
    "                           shape=(n_samples,),\n",
    "                           chunks=(1000,),\n",
    "                           fletcher32=True, \n",
    "                           dtype='float32')\n",
    "    \n",
    "    h5_file['brcs'][:] = ds.brcs.values\n",
    "    h5_file['windspeed'][:] = ds.windspeed.values\n",
    "    h5_file.flush()\n",
    "    h5_file.attrs['timestamp'] = str(datetime.datetime.now())\n",
    "    run_time = (time.time() - start_time)\n",
    "    print(f'{n_samples} samples appended to file '\n",
    "          f'{h5_file.filename} in {run_time:.2f} seconds')\n",
    "    h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ds_to_hdf5(ds_train, 'train_data.h5')\n",
    "save_ds_to_hdf5(..., ...) # repeat for the valid dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to open a hdf5 file in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_hdf5 = h5py.File('train_data.h5', 'r')\n",
    "print(ds_train_hdf5)\n",
    "print(ds_train_hdf5.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to load a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_train_hdf5['brcs'][:];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will look in detail at the two classes that are needed in PyTorch for feeding data to the neural network:\n",
    "* Dataset\n",
    "* DataLoader\n",
    "\n",
    "The Dataset for CyGNSS data is defined in the following cell. \n",
    "\n",
    "**TASK** Look at the class definition below. Generate the train and the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyGNSSDataset(Dataset):\n",
    "    def __init__(self, flag):\n",
    "        '''\n",
    "        Load data from hdf5 file\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        flag : string\n",
    "            Any of train / valid / test. Defines dataset.\n",
    "        -----------\n",
    "        Returns: dataset\n",
    "        '''\n",
    "        self.h5_file = h5py.File(flag + '_data.h5', 'r', rdcc_nbytes=0)  # disable cache\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.y = self.h5_file['windspeed'][:].astype(np.float32)\n",
    "        self.X = self.h5_file['brcs'][:].astype(np.float32)\n",
    "\n",
    "        print(f'load {flag} input data: {self.X.shape} ({self.X.nbytes // 1e6}MB)')\n",
    "        print(f'load {flag} labels: {self.y.shape} ({self.y.nbytes // 1e6}MB)')\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''required function for the pytorch dataloader: returns len(samples)'''\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''required function for the pytorch dataloader: yields sample at idx'''\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ...\n",
    "valid_dataset = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataLoader` takes a pytorch `Dataset` and produces minibatches. \n",
    "\n",
    "**TASK** Generate a `DataLoader` for the train and validation dataset each. Look up the documentation of the `DataLoader` and discuss it / write a question about it in the google doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = Dataloader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_dataloader = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X, y) in enumerate(train_dataloader):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 bleeding edge (using the module anaconda3/bleeding_edge)",
   "language": "python",
   "name": "anaconda3_bleeding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
