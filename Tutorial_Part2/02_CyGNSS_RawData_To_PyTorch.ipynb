{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GFZ Notebook\n",
    "(zu viele Rohdaten → filtern und speichern)\n",
    "Datei öffnen, plotten, Verteilungen, NANs entfernen, outlier removal, trends,  basierend auf hässlichem GFZ (Caroline)\n",
    "PyTorch Dataset\n",
    "Vollständig definiert vorhanden\n",
    "z.B. weitere Variable hinzufügen\n",
    "sollte Dokumentation verwenden\n",
    "Kompletter ML Workflow (laden / preprocessing / training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing CyGNSS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: generate large test data, save in multiple netcdf files --> open_mfdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('cygnss_testdata.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "    - Plot some of the variables\n",
    "    - Filter the variables by applying the quality flag\n",
    "    - Try to form a minibatch by randomly reading from these netcdf files. Time the execution. Get an estimate how long one epoch would take (--> inefficient to read from netcdf files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save as hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: adapt the function xarray-->hdf5 from cygnss git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "    - save the datasets as {train,valid,test}_data.h5 by executing the provided function\n",
    "    - Again try to form a minibatch by randomly reading from these files. Time the execution. Is there a speedup? (--> it is good practice to save the data in a format that is useful for your later application)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyGNSSDataset(Dataset):\n",
    "    def __init__(self, flag, input_v_map=['brcs'], normalization_values=None, filter_quality=False):\n",
    "        '''\n",
    "        Load data and apply transforms during setup\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        flag : string\n",
    "            Any of train / valid / test. Defines dataset.\n",
    "        input_v_map : list\n",
    "            Input maps, choice of ['brcs', 'eff_scatter']\n",
    "        normalization_values : dict\n",
    "            Mean and standard deviation, needed for scaling the input variables\n",
    "        filter_quality : bool\n",
    "            Filter samples that are flagged as bad quality (default: False)\n",
    "        -----------\n",
    "        Returns: dataset\n",
    "        '''\n",
    "        self.h5_file = h5py.File(os.path.join('/work/ka1176/shared_data/CyGNSS/', flag + '_data.h5'), 'r', rdcc_nbytes=0)  # disable cache\n",
    "        # load everything into memory\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # load labels\n",
    "        self.y = self.h5_file['windspeed'][:].astype(np.float32)\n",
    "\n",
    "        # normalize main input data\n",
    "        # Save normalization values together with the trained model\n",
    "        # For inference load the normalization values\n",
    "\n",
    "        if flag=='train': # determine normalization values\n",
    "            self.normalization_values = dict()\n",
    "        else:\n",
    "            self.normalization_values = normalization_values\n",
    "        \n",
    "        # stack map vars (2D vars)\n",
    "        self.X = []\n",
    "        for v_map in input_v_map:\n",
    "            X_v_map = self.h5_file[v_map][:].astype(np.float32)\n",
    "            \n",
    "            if flag=='train':\n",
    "                norm_vals = dict()\n",
    "                X_v_map_scaled, X_mean, X_std = self._standard_scale(X_v_map)\n",
    "                self.normalization_values[f'{v_map}_mean'] = X_mean\n",
    "                self.normalization_values[f'{v_map}_std']  = X_std\n",
    "            else:\n",
    "                X_mean = self.normalization_values[f'{v_map}_mean']\n",
    "                X_std = self.normalization_values[f'{v_map}_std']\n",
    "                X_v_map_scaled = self._standard_scale_given(X_v_map, X_mean, X_std)\n",
    "                \n",
    "            self.X.append(X_v_map_scaled) # append scaled 2D map\n",
    "        self.X = np.stack(self.X, axis=1)\n",
    "        \n",
    "        if filter_quality:\n",
    "            n_before = len(self.y)\n",
    "            mask = self.h5_file['quality'][:]\n",
    "            self.X, self.y = self.X[mask], self.y[mask]\n",
    "            print(f'After filter_quality, {len(self.y)} samples remain ({len(self.y)/n_before*100:.1f}%)')\n",
    "\n",
    "        print(f'load and transform {flag} input data: {self.X.shape} ({self.X.nbytes // 1e6}MB)')\n",
    "        print(f'load and transform {flag} labels: {self.y.shape} ({self.y.nbytes // 1e6}MB)')\n",
    "        \n",
    "    def _standard_scale(self, v):\n",
    "        '''apply standard scale and return mean / std'''\n",
    "        mean = np.mean(v)\n",
    "        sigma = np.std(v)\n",
    "        v_tilde = (v - mean) / sigma\n",
    "        return v_tilde, mean, sigma\n",
    "    \n",
    "    def _standard_scale_given(self, v, mean, sigma):\n",
    "        '''apply standard scale with pre-determined mean / std'''\n",
    "        v_tilde = (v - mean) / sigma\n",
    "        return v_tilde\n",
    "\n",
    "    def _filter_all_data_by_mask(self, mask, flag, name=''): \n",
    "        '''filter the input data by the provided mask'''\n",
    "        self.X, self.y = self.X[mask], self.y[mask]\n",
    "        print(f'{flag} input data after {name} downsampling: {self.X.shape} ({self.X.nbytes // 1e6}MB)')\n",
    "\n",
    "    def __len__(self):\n",
    "        '''required function for the pytorch dataloader'''\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''required function for the pytorch dataloader'''\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        return (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataloaders(filter_quality=False, input_v_map=['brcs']):\n",
    "    '''Load the datasets and create PyTorch dataloaders\n",
    "    \n",
    "    Input parameters:\n",
    "    -------------------------\n",
    "    filter_quality : apply a filter for sample quality (default: False)\n",
    "    input_v_map    : list of input features (default: ['brcs'])\n",
    "    -------------------------\n",
    "    \n",
    "    Returns:\n",
    "    -------------------------\n",
    "    pytorch DataLoader instances for train / validation / test set\n",
    "    '''\n",
    "    \n",
    "    train_dataset = CyGNSSDataset('train', filter_quality=filter_quality, input_v_map=input_v_map)\n",
    "    valid_dataset = CyGNSSDataset('valid', filter_quality=filter_quality, input_v_map=input_v_map, normalization_values=train_dataset.normalization_values)\n",
    "    test_dataset = CyGNSSDataset('test', filter_quality=filter_quality, input_v_map=input_v_map, normalization_values=train_dataset.normalization_values)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    test_dataloader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks:\n",
    "    - iterate through samples by iterating a dataloader (python concept \"yield\")\n",
    "    - pass an argument to the dataset to add another variable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
